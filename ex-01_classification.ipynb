{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91840d1",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The goal of this interactive demo is to show you how a machine learning model can perform classification. However, keep in mind, that the code in this notebook was simplified for the demo, and should not be used as a plug and play example for real machine learning projects.\n",
    "\n",
    "In this notebook we will explore three different types of classification models:\n",
    "\n",
    "- Decision trees\n",
    "- K-nearest neighbors\n",
    "- Support vecotr machines\n",
    "\n",
    "## The dataset\n",
    "\n",
    "For the purpose of this demo we will use the [palmer penguin dataset](https://allisonhorst.github.io/palmerpenguins/), and only look at two relevant features. Using only two features, allows us to plot the dataset in a 2-dimensional figure and color code the background according to the target class - keep in mind that this is only possible for such low-dimensional datasets, and not something that is otherwise routinely done in machine learning projects.\n",
    "\n",
    "Let's go ahead and download and prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load penguins dataset\n",
    "df = pd.read_csv('penguins.csv')\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Show 5 random rows in the dataset\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf40df",
   "metadata": {},
   "source": [
    "Let's extract the size of the dataset and report how many penguins were recorded for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report size of the dataset\n",
    "print(f\"The dataset contains {df.shape[0]} entries, with {df.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258baceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report class distribution\n",
    "df.species.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc1ff0",
   "metadata": {},
   "source": [
    "For our classification demo, we are only focused on 'bill length' and 'bill depth'. So let's reduce the dataset to only what we need: A feature matrix $X$ and a target vector $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature matrix X\n",
    "X = df[['bill_length_mm', 'bill_depth_mm']]\n",
    "X.columns = [\"Bill length [mm]\", \"Bill depth [mm]\"]\n",
    "\n",
    "# Extract target vector y\n",
    "y = df['species']\n",
    "species = y.unique()\n",
    "y = y.map({'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63a726",
   "metadata": {},
   "source": [
    "Before we train a classifier on our dataset, let's first take a look at it. To fasciliate this, we prepared a small plotting routine for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79552ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_data\n",
    "\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6aaed",
   "metadata": {},
   "source": [
    "Great, everything is ready for the classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfce41f2",
   "metadata": {},
   "source": [
    "## Classification using Decision Trees\n",
    "\n",
    "Let's start with a rule-based classifier - the **if-then-else** decision trees. And as this is a simple demo, let's only focus on one single hyper-parameter - the **maximum depth of the tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56714e53-9759-449c-aa20-3a926e28c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spefiy maximum depth of the tree\n",
    "max_depth = 1\n",
    "\n",
    "# Train decision tree classifier\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "%time clf.fit(X, y)\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61141a1b-3165-4f50-99e0-733a2ed4dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "tree.plot_tree(clf, feature_names=X.columns, filled=True,\n",
    "               proportion=True, class_names=species, impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7d4cf",
   "metadata": {},
   "source": [
    "And to better understand how this decision tree partitions the 2-dimensional plane of the dataset, let's plot the dataset together with the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundaries\n",
    "\n",
    "plot_decision_boundaries(\n",
    "    X, y, clf, species, title=f\"Decision Tree with depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbc9a8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h2>Exercise</h2>\n",
    "    <p></p>\n",
    "Change the <code>max_depth</code> parameter above to something between 1 to 100, and rerun the two code cells. Based on the outputs, what do you think is the best value for <code>max_depth</code>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bf11c",
   "metadata": {},
   "source": [
    "## Classification using K-nearest neighbors\n",
    "\n",
    "Next, let's take a look at a k-nearest neighbors model. Once more, to keep things simple, let's only look at one single hyper-parameter - the **number of neighbors** to consider for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7174761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spefiy number of neighbors to consider\n",
    "n_neighbors = 1\n",
    "\n",
    "# Train k-nearest neighbour classifier\n",
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n",
    "%time clf.fit(X, y)\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46831a57",
   "metadata": {},
   "source": [
    "One the model is trained, we can once more plot the decision boundaries for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce50035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundaries\n",
    "\n",
    "plot_decision_boundaries(X, y, clf, species, title=f\"KNN with {n_neighbors} neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ee4c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h2>Exercise</h2>\n",
    "    <p></p>\n",
    "Change the <code>n_neighbors</code> parameter above to something between 1 to 300 (the higher the number, the longer the computation), and rerun the two code cells. Based on the outputs, what do you think is the best value for <code>n_neighbors</code>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6c5ba",
   "metadata": {},
   "source": [
    "## Classification using Support Vector Machines\n",
    "\n",
    "Last but certainly not least, let's take a look at a support vector machine (SVM) model. This time, let's look at two hyper-parameter - the **regularization parameter `C`** and **rbf-kernel parameter `gamma`**  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51255aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spefiy regularization and rbf-kernel parameter\n",
    "C = 1\n",
    "gamma = 0.1\n",
    "\n",
    "# Train support vector machine classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel=\"rbf\", C=C, gamma=gamma)\n",
    "%time clf.fit(X, y)\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99f454",
   "metadata": {},
   "source": [
    "One the model is trained, we can once more plot the decision boundaries for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundaries\n",
    "\n",
    "plot_decision_boundaries(X, y, clf, species, scaled=True, title=\"SVM with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03d558",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h2>Exercise</h2>\n",
    "    <p></p>\n",
    "Change the <code>C</code> and <code>gamma</code> parameter above to something between 0.0001 and 1000 (we recommend to use factors of 10, i.e. 0.01, 0.1, 1, 10,...), and rerun the two code cells. Based on the outputs, what do you think are the best values for <code>C</code> and <code>gamma</code>?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
