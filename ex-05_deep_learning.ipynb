{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8efff1",
   "metadata": {},
   "source": [
    "# Deep learning\n",
    "\n",
    "The goal of this interactive demo is to show you how a deep learning model can be setup, in this case using [Google's TensorFlow](https://tensorflow.org/) package. More precisely, we will establish a convolutional neural network that is able to differentiate between 10 different object classes. However, keep in mind, that the code in this notebook was simplified for the demo, and should not be used as a plug and play example for real machine learning projects.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "For the purpose of this demo we will use the [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. This dataset contains 60'000 color images (of size 32 x 32 pixels) of 10 distinct target classes:\n",
    "\n",
    "1. airplane\n",
    "2. automobile\n",
    "3. bird\n",
    "4. cat\n",
    "5. deer\n",
    "6. dog\n",
    "7. frog\n",
    "8. horse\n",
    "9. ship\n",
    "10. truck\n",
    "\n",
    "So let's go ahead and download and prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant tensorflow package\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load dataset, already pre-split into train and test set\n",
    "(X_tr, y_tr), (X_te, y_te) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale images to a range between -0.5 and +0.5\n",
    "X_tr = (X_tr.astype(\"float32\") - 128) / 255.0\n",
    "X_te = (X_te.astype(\"float32\") - 128) / 255.0\n",
    "\n",
    "# Reduce footprint for demo to manage memory restrictions\n",
    "X_tr, y_tr = X_tr[::4], y_tr[::4]\n",
    "X_te, y_te = X_te[::8], y_te[::8]\n",
    "\n",
    "# Report shape of dataset\n",
    "print(\"X_tr shape:\", X_tr.shape)\n",
    "print(\"X_te shape:\", X_te.shape)\n",
    "\n",
    "print(\"\\nData is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f89fa",
   "metadata": {},
   "source": [
    "Let's have a look at the first few hundered images of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create image collage\n",
    "img_collage = np.concatenate(\n",
    "    [np.concatenate([X_tr[idx + jdx * 25] for idx in range(25)], axis=1)\n",
    "     for jdx in range(10)])\n",
    "\n",
    "# Rescale image for visualization purpose\n",
    "img_collage = (255 * (img_collage + 0.5)).astype(\"uint8\")\n",
    "\n",
    "# Plot image collage\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_collage)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a5f6a",
   "metadata": {},
   "source": [
    "## The neural network model\n",
    "\n",
    "Now that the data is ready, let's go ahead and create the convolutional neural network, or short the `ConvNet`. The architecture of a `ConvNet` consists of two parts:\n",
    "\n",
    "1. The **convolutional layers** that will help to extract meaning full features from the dataset.\n",
    "2. The **fully connected dense layers** that will combine the extracted features in non-linear ways to perform the model prediction.\n",
    "\n",
    "The following model is one way to implement such an architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create input layer\n",
    "input_layer = layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "# Create first convolutional layer (with some extra flavors)\n",
    "x = layers.Conv2D(64, kernel_size=5, strides=2, padding='same')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Create second convolutional layer (with some extra flavors)\n",
    "x = layers.Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "# Flatten everything to allow the transition to the fully connected dense layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create fully connected dense layer (with some extra flavors)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "# Create output layer\n",
    "x = layers.Dense(10)(x)\n",
    "output_layer = layers.Activation(\"softmax\")(x)\n",
    "\n",
    "# Create model based on input and output layer\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"ConvNet\")\n",
    "\n",
    "print(\"\\nModel is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6dce0",
   "metadata": {},
   "source": [
    "Once the model is created, we can use the `summary()` method to get an overview of the network's architecture\n",
    "and the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d913aa1",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now that the data and the model are ready, we can go ahead and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eef072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify some additional model training parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Compile model with appropriate metrics and optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_tr, y_tr, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "print(\"\\nModel finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206844cd",
   "metadata": {},
   "source": [
    "## Model investigation\n",
    "\n",
    "Once the model has finished training, we can investigate a few interesting things. First, how did the model performance improve over training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebdba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_convnet_history\n",
    "plot_convnet_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda34aae",
   "metadata": {},
   "source": [
    "Ok, it seems that the scores on the training and the validation set, has improved over time. That's great! And how good is our model at the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facaa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance accuracy on training and test set\n",
    "_, acc_tr = model.evaluate(X_tr, y_tr, verbose=1)\n",
    "_, acc_te = model.evaluate(X_te, y_te, verbose=1)\n",
    "\n",
    "# Report scores\n",
    "print(f\"Train accuracy: {acc_tr*100:.2f}%\")\n",
    "print(f\"Test accuracy:  {acc_te*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7937c6",
   "metadata": {},
   "source": [
    "Having one single performance metric is often difficult to interpret. So let's go a step further and have a look at the confusion matrix. In this matrix we can see with which target class the model confused a true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e690950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model predictions\n",
    "y_pred = model.predict(X_te, verbose=0)\n",
    "\n",
    "# Transform class probabilities to prediction labels\n",
    "predictions = np.argmax(y_pred, 1)\n",
    "\n",
    "# Create confusion matrix\n",
    "import pandas as pd\n",
    "class_labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "                \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "from tensorflow.math import confusion_matrix\n",
    "cm = confusion_matrix(y_te, predictions)\n",
    "cm = pd.DataFrame(cm.numpy(), columns=class_labels, index=class_labels)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, square=True, annot=True, fmt=\"d\", cbar=False, cmap=\"Spectral_r\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b24efa",
   "metadata": {},
   "source": [
    "As we can see, our model has problem to destinguish cats from dogs, but no issues with airplanes and automobiles or ships and trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fd7ef",
   "metadata": {},
   "source": [
    "# Model visualization\n",
    "\n",
    "The following model investigation is not always done, or if so, in a much more efficient way. But to better highlight the feature extraction capability of deep learning models, let's have a closer look at the convolutional filters that our model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f472320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first convolutional layer of our model\n",
    "from utils import plot_convnet_weights\n",
    "plot_convnet_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a2eb0",
   "metadata": {},
   "source": [
    "Each image that we pass through our model will first be filtered by one of these 64 convolutional filters. Therefore, each of these 64 filters is one way how our model \"see's the world\". So let's take an image from our dataset and visualize all 64 different ways these filters interpret the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random image from the test set\n",
    "img = X_te[6]\n",
    "\n",
    "# Plot random image\n",
    "img -= img.min()\n",
    "img /= img.max()\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_convnet_activation_map\n",
    "plot_convnet_activation_map(img, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
