{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a23248",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "The goal of this interactive demo is to show you how a machine learning model can perform clustering. However, keep in mind, that the code in this notebook was simplified for the demo, and should not be used as a plug and play example for real machine learning projects.\n",
    "\n",
    "In this notebook we will explore three different types of clustering approaches:\n",
    "\n",
    "- Centroid based\n",
    "- Density based\n",
    "- Connectivity based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac826873",
   "metadata": {},
   "source": [
    "## Centroid based clustering\n",
    "\n",
    "Centroid based clustering is one of the more \"simple\" and straight to the point clustering. You specify how many clusters you want to have and the model will partition your data into exactly as many clusters.\n",
    "\n",
    "Let's take a look at a rather simple dataset, a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image of a flower\n",
    "import imageio as io\n",
    "img = io.v2.imread(\"flower.jpg\") / 255.0\n",
    "print(f\"The image has a shape of {img.shape}.\")\n",
    "\n",
    "# Plot image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f6448",
   "metadata": {},
   "source": [
    "Very beautiful! Now, as we can see, this color image has a size of 214 x 320 pixels. So let's consider each of this pixels as an individual data point, and the three RGB color channels red, green and blue, as the datasets features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image to 2-dimensional dataset\n",
    "X = img.reshape(-1, 3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e17163",
   "metadata": {},
   "source": [
    "Given that this dataset only has 3 dimensions, let's go ahead and visualize each pixel and their corresponding color value in a 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2676e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_rgb_space\n",
    "plot_rgb_space(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ffa80",
   "metadata": {},
   "source": [
    "Great! So we can see that we have a lot of green and red colors,dark and bright but not much of blue. So let's go ahead and use a centroid based clustering approach to partiion this RGB color space into N clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f21d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "n_clusters = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da658792",
   "metadata": {},
   "source": [
    "For the centroid clustering routine we will be using `KMeans`, a simple but efficent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "# Fit model to data\n",
    "%time kmeans.fit(X)\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866bc92",
   "metadata": {},
   "source": [
    "Once the model is trained we can take a closer look at the centroids it found. In our case, these centroids actually represent points in the RGB-color space and as such can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of RGB-color centroids\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.imshow(kmeans.cluster_centers_[None, ...], aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b551390",
   "metadata": {},
   "source": [
    "Going one step further, we could now replace any of the $256^3$ color combinations in the original image, by the closest centroid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute closest centroid label\n",
    "centroid_labels = kmeans.predict(X)\n",
    "\n",
    "# Reshape centroid labels back into an image\n",
    "img_centroid = kmeans.cluster_centers_[centroid_labels].reshape(img.shape)\n",
    "\n",
    "# Plot centroid labeled image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img_centroid)\n",
    "plt.title(f\"Quantized image using {n_clusters} centroids\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cfdab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h2>Exercise</h2>\n",
    "    <p></p>\n",
    "Change the <code>n_clusters</code> parameter above to anything between 1 and 1000 and rerun all the code  after that. How does this effect the quantized image here? Is there an ideal sweet spot of number of clusters?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695ef44",
   "metadata": {},
   "source": [
    "## Density based clustering\n",
    "\n",
    "For the density based clustering approach, let's take a different dataset. Mostly because these algorithms sometimes struggle when the datapoints are too close (i.e. dense) to each other, as can be seen in the 3-dimensional RGB plots above.\n",
    "\n",
    "To simplify things, let's quickly create a synthethic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthethic dataset\n",
    "from utils import create_synthethic_dataset\n",
    "\n",
    "X = create_synthethic_dataset(n_points_per_cluster=250)\n",
    "\n",
    "# Visualize snythethic dataset\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(*X.T, s=10, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf5b77",
   "metadata": {},
   "source": [
    "The great thing about density based clustering routines is that we don't have to specify how many clusters we want to extract, as this is not always easy to know. However what we need to specify are the density criteria to classify a region as dense enough to be a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dcf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create density estimator of type OPTICS\n",
    "from sklearn.cluster import OPTICS\n",
    "clust = OPTICS(min_samples=50, xi=0.05, min_cluster_size=0.05)\n",
    "\n",
    "# Train the density clustering model\n",
    "%time clust.fit(X)\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f61088",
   "metadata": {},
   "source": [
    "Once the model is trained we can take a closer look at which clusters it found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477770d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data with cluster labels\n",
    "plt.figure(figsize=(7, 7))\n",
    "for idx in range(len(set(clust.labels_)) - 1):\n",
    "    Xk = X[clust.labels_ == idx]\n",
    "    plt.scatter(Xk[:, 0], Xk[:, 1], s=10, alpha=0.5, label=idx)\n",
    "plt.plot(X[clust.labels_ == -1, 0],\n",
    "         X[clust.labels_ == -1, 1],\n",
    "         \"k+\", alpha=0.2, label=\"outlier\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349d4fa",
   "metadata": {},
   "source": [
    "To better understand how the density algorithm identified certain points as outliers, let's take a look at the density of the individual clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9014dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract important cluster properties\n",
    "import numpy as np\n",
    "\n",
    "space = np.arange(len(X))\n",
    "reachability = clust.reachability_[clust.ordering_]\n",
    "labels = clust.labels_[clust.ordering_]\n",
    "\n",
    "# Create reachability plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "for idx in range(len(set(clust.labels_)) - 1):\n",
    "    Xk = space[labels == idx]\n",
    "    Rk = reachability[labels == idx]\n",
    "    plt.scatter(Xk, Rk, alpha=0.5, label=idx)\n",
    "plt.plot(space[labels == -1], reachability[labels == -1],\n",
    "         \"k.\", alpha=0.2, label=\"outlier\")\n",
    "plt.ylabel(\"Reachability (epsilon distance)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ca3b6",
   "metadata": {},
   "source": [
    "## Connectivity based clustering\n",
    "\n",
    "Last but not least, let's take a look at a connectivity based clustering routine. As for the other approaches, there are multiple models that can perform this task. Each with different advantages and disadvantages.\n",
    "\n",
    "For this example, we will simplified version of the MNIST dataset and as a connectivity based clustering model, we will be using `AgglomerativeClustering`.\n",
    "\n",
    "So let's start with preparing the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8fbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset from file\n",
    "data = np.load('digits.npy')\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8900d",
   "metadata": {},
   "source": [
    "The digits dataset contains multiple hand written examples of the digits from 0 to 9. Once the dataset dimension was reduces to two, we can easily visualize it in a nice plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_clustering\n",
    "plot_clustering(X, y, y, title=\"Ground truth / Correct labeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af44d",
   "metadata": {},
   "source": [
    "Let's now use a `AgglomerativeClustering` model to cluster this 2-dimensional dataset. While there are multiple parameters that we could tweak, let's only manipualte the `distance_threshold` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linkage distance threshold above which, clusters will not be merged\n",
    "distance_threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54059510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AgglomerativeClustering model\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "model = AgglomerativeClustering(distance_threshold=distance_threshold, n_clusters=None)\n",
    "\n",
    "# Train model on dataset\n",
    "%time model.fit(X)\n",
    "\n",
    "print(\"\\nFinished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e95f3",
   "metadata": {},
   "source": [
    "Once the connectivity based clustering model is trained, we can go ahead and plot the original dataset, and color code the predicted cluster labels individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering(X, y, model.labels_, \"Detected clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acc5ec",
   "metadata": {},
   "source": [
    "The great thing about connectivity based clustering routines is that they allow us to perform hierarchical clustering. In other words, the model can tell us which clusters are closest to each other and potentially could be merged, or how a big cluster could be split into smaller ones.\n",
    "\n",
    "A great way to visualize this inter-dependence between samples and clusters are dendrograms.\n",
    "\n",
    "Note, the color coding in the following figure doesn't correspond to the colors in number plot above! But what it shows is the 8 clusters, each represented by two nodes (i.e. two smaller clusters) and how these 8 clusters hierarchically combine into the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram\n",
    "from utils import plot_dendrogram\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=3, color_threshold=distance_threshold)\n",
    "plt.xlabel(\"Number of points in node.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438191e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h2>Exercise</h2>\n",
    "    <p></p>\n",
    "Change the <code>distance_threshold</code> parameter above to anything between 0 and 0.1 (see the y-axis in the dendrogram to know where to cut) and rerun the code  after that. How does this effect the clustering?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
